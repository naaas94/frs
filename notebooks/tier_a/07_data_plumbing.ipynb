{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pattern 7: Data Plumbing\n",
        "\n",
        "**The Interview-Isomorphic Pattern**\n",
        "\n",
        "This is the hidden pattern behind most \"real-world\" coding challenges.\n",
        "\n",
        "---\n",
        "\n",
        "## The Universal Pipeline\n",
        "\n",
        "Every data plumbing problem is the same pattern in different disguises:\n",
        "\n",
        "```\n",
        "parse → validate → normalize → aggregate → select top-k/best → report errors\n",
        "```\n",
        "\n",
        "**Time**: O(n) single pass or O(n log k) with top-k selection  \n",
        "**Space**: O(n) for aggregated data\n",
        "\n",
        "---\n",
        "\n",
        "## When to Use\n",
        "\n",
        "- JSONL/CSV file processing\n",
        "- API response handling\n",
        "- Log aggregation\n",
        "- ETL-style transforms\n",
        "- \"Process this messy data\" problems\n",
        "\n",
        "**Recognition trigger**: \"Given a file/stream of records...\", \"aggregate by...\", \"handle malformed...\"\n",
        "\n",
        "---\n",
        "\n",
        "## Pattern Template\n",
        "\n",
        "```python\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from typing import Iterator, Dict, List, Any\n",
        "\n",
        "def process_records(lines: Iterator[str]) -> Dict[str, Any]:\n",
        "    \"\"\"Universal plumbing template.\"\"\"\n",
        "    aggregated = defaultdict(list)\n",
        "    errors = []\n",
        "    \n",
        "    for line_num, line in enumerate(lines, 1):\n",
        "        # 1. PARSE\n",
        "        try:\n",
        "            record = json.loads(line.strip())\n",
        "        except json.JSONDecodeError as e:\n",
        "            errors.append({\"line\": line_num, \"error\": \"parse\", \"msg\": str(e)})\n",
        "            continue\n",
        "        \n",
        "        # 2. VALIDATE\n",
        "        if not validate(record):\n",
        "            errors.append({\"line\": line_num, \"error\": \"validation\", \"record\": record})\n",
        "            continue\n",
        "        \n",
        "        # 3. NORMALIZE\n",
        "        normalized = normalize(record)\n",
        "        \n",
        "        # 4. AGGREGATE\n",
        "        key = normalized[\"group_key\"]\n",
        "        aggregated[key].append(normalized)\n",
        "    \n",
        "    # 5. SELECT (top-k, best, filter)\n",
        "    result = select_top(aggregated, k=10)\n",
        "    \n",
        "    # 6. REPORT ERRORS\n",
        "    return {\"data\": result, \"errors\": errors, \"error_count\": len(errors)}\n",
        "```\n",
        "\n",
        "## Invariant Statement\n",
        "\n",
        "After processing line `i`, the aggregation state correctly reflects all valid records from lines `[1, i]`, and all invalid records are captured in the error list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import json\n",
        "import csv\n",
        "from io import StringIO\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Dict, Any, Iterator, Optional\n",
        "from dataclasses import dataclass\n",
        "import heapq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Walkthrough: JSONL Log Aggregation\n",
        "\n",
        "**Problem**: Given a JSONL file of API logs, compute:\n",
        "1. Request count per endpoint\n",
        "2. Top-5 slowest endpoints by avg latency\n",
        "3. Error rate per endpoint\n",
        "4. Handle malformed lines gracefully\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample JSONL data\n",
        "SAMPLE_LOGS = \"\"\"\n",
        "{\"endpoint\": \"/api/users\", \"latency_ms\": 120, \"status\": 200}\n",
        "{\"endpoint\": \"/api/orders\", \"latency_ms\": 350, \"status\": 200}\n",
        "{\"endpoint\": \"/api/users\", \"latency_ms\": 95, \"status\": 200}\n",
        "{\"endpoint\": \"/api/orders\", \"latency_ms\": 500, \"status\": 500}\n",
        "{\"endpoint\": \"/api/products\", \"latency_ms\": 80, \"status\": 200}\n",
        "malformed line here\n",
        "{\"endpoint\": \"/api/users\", \"latency_ms\": 110, \"status\": 404}\n",
        "{\"missing_endpoint\": true}\n",
        "{\"endpoint\": \"/api/products\", \"latency_ms\": 75, \"status\": 200}\n",
        "\"\"\".strip().split('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_logs(lines: List[str]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Process API logs following the universal pipeline.\n",
        "    \n",
        "    INVARIANT: After each line, stats[endpoint] reflects\n",
        "    all valid requests seen for that endpoint.\n",
        "    \"\"\"\n",
        "    # Aggregation state\n",
        "    stats = defaultdict(lambda: {\"count\": 0, \"total_latency\": 0, \"errors\": 0})\n",
        "    parse_errors = []\n",
        "    validation_errors = []\n",
        "    \n",
        "    for line_num, line in enumerate(lines, 1):\n",
        "        # 1. PARSE\n",
        "        try:\n",
        "            record = json.loads(line.strip())\n",
        "        except json.JSONDecodeError:\n",
        "            parse_errors.append(line_num)\n",
        "            continue\n",
        "        \n",
        "        # 2. VALIDATE (required fields)\n",
        "        required = [\"endpoint\", \"latency_ms\", \"status\"]\n",
        "        if not all(k in record for k in required):\n",
        "            validation_errors.append({\"line\": line_num, \"record\": record})\n",
        "            continue\n",
        "        \n",
        "        # 3. NORMALIZE (already clean in this example)\n",
        "        endpoint = record[\"endpoint\"]\n",
        "        latency = record[\"latency_ms\"]\n",
        "        is_error = record[\"status\"] >= 400\n",
        "        \n",
        "        # 4. AGGREGATE\n",
        "        stats[endpoint][\"count\"] += 1\n",
        "        stats[endpoint][\"total_latency\"] += latency\n",
        "        if is_error:\n",
        "            stats[endpoint][\"errors\"] += 1\n",
        "    \n",
        "    # 5. SELECT (compute derived metrics + top-k)\n",
        "    results = []\n",
        "    for endpoint, data in stats.items():\n",
        "        avg_latency = data[\"total_latency\"] / data[\"count\"]\n",
        "        error_rate = data[\"errors\"] / data[\"count\"]\n",
        "        results.append({\n",
        "            \"endpoint\": endpoint,\n",
        "            \"count\": data[\"count\"],\n",
        "            \"avg_latency_ms\": round(avg_latency, 2),\n",
        "            \"error_rate\": round(error_rate, 3)\n",
        "        })\n",
        "    \n",
        "    # Top 5 by average latency (descending)\n",
        "    top_slow = sorted(results, key=lambda x: -x[\"avg_latency_ms\"])[:5]\n",
        "    \n",
        "    # 6. REPORT\n",
        "    return {\n",
        "        \"by_endpoint\": results,\n",
        "        \"top_slow\": top_slow,\n",
        "        \"parse_errors\": parse_errors,\n",
        "        \"validation_errors\": validation_errors\n",
        "    }\n",
        "\n",
        "# Run\n",
        "result = aggregate_logs(SAMPLE_LOGS)\n",
        "print(\"By endpoint:\", result[\"by_endpoint\"])\n",
        "print(\"Top slow:\", result[\"top_slow\"])\n",
        "print(\"Parse errors on lines:\", result[\"parse_errors\"])\n",
        "print(\"Validation errors:\", result[\"validation_errors\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drill Problems\n",
        "\n",
        "### Drill 1: CSV Validation\n",
        "Given a CSV with columns `name,email,age`, validate:\n",
        "- Email contains \"@\"\n",
        "- Age is a positive integer\n",
        "- Return valid rows + list of errors with line numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAMPLE_CSV = \"\"\"name,email,age\n",
        "Alice,alice@example.com,30\n",
        "Bob,bob-at-email,25\n",
        "Charlie,charlie@test.org,-5\n",
        "Diana,diana@corp.io,28\n",
        ",missing@name.com,22\n",
        "Eve,eve@place.net,not_a_number\n",
        "\"\"\"\n",
        "\n",
        "def validate_csv(csv_content: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    TODO: Implement CSV validation.\n",
        "    Return: {\"valid\": [...], \"errors\": [{\"line\": N, \"reason\": \"...\"}]}\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# Test your implementation\n",
        "# result = validate_csv(SAMPLE_CSV)\n",
        "# print(\"Valid:\", result[\"valid\"])\n",
        "# print(\"Errors:\", result[\"errors\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Drill 2: API Response Merger\n",
        "Given paginated API responses as a list of dicts, merge them and deduplicate by ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PAGES = [\n",
        "    {\"data\": [{\"id\": 1, \"name\": \"A\"}, {\"id\": 2, \"name\": \"B\"}], \"next\": True},\n",
        "    {\"data\": [{\"id\": 2, \"name\": \"B\"}, {\"id\": 3, \"name\": \"C\"}], \"next\": True},\n",
        "    {\"data\": [{\"id\": 4, \"name\": \"D\"}], \"next\": False},\n",
        "]\n",
        "\n",
        "def merge_pages(pages: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    TODO: Merge all pages, deduplicate by ID (keep first occurrence).\n",
        "    Return list of unique records.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# Expected: [{\"id\": 1, \"name\": \"A\"}, {\"id\": 2, \"name\": \"B\"}, {\"id\": 3, \"name\": \"C\"}, {\"id\": 4, \"name\": \"D\"}]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Variants\n",
        "\n",
        "| Variant | Key Difference |\n",
        "|---------|----------------|\n",
        "| Streaming | Use generators, don't load all into memory |\n",
        "| Top-K | Use heap instead of sorting at end |\n",
        "| Window aggregation | Keep rolling window state |\n",
        "| Multi-file | Outer loop over files, same inner logic |\n",
        "\n",
        "## Edge Case Checklist\n",
        "\n",
        "- [ ] Empty file/input\n",
        "- [ ] All records invalid\n",
        "- [ ] First/last line malformed\n",
        "- [ ] Unicode/encoding issues\n",
        "- [ ] Very large numbers (overflow?)\n",
        "- [ ] Null/None values in fields\n",
        "- [ ] Whitespace in data\n",
        "\n",
        "## Common Bugs\n",
        "\n",
        "| Bug | Fix |\n",
        "|-----|-----|\n",
        "| Not stripping whitespace | `.strip()` before parse |\n",
        "| Swallowing all exceptions | Catch specific exceptions, log others |\n",
        "| Off-by-one line numbers | `enumerate(lines, 1)` for 1-indexed |\n",
        "| Division by zero in averages | Check `count > 0` before divide |\n",
        "| Mutating input data | Work on copies if needed |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solutions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solutions\n",
        "\n",
        "def validate_csv_solution(csv_content: str) -> Dict[str, Any]:\n",
        "    reader = csv.DictReader(StringIO(csv_content.strip()))\n",
        "    valid = []\n",
        "    errors = []\n",
        "    \n",
        "    for line_num, row in enumerate(reader, 2):  # 2 because header is line 1\n",
        "        issues = []\n",
        "        \n",
        "        # Validate name\n",
        "        if not row.get(\"name\", \"\").strip():\n",
        "            issues.append(\"missing name\")\n",
        "        \n",
        "        # Validate email\n",
        "        if \"@\" not in row.get(\"email\", \"\"):\n",
        "            issues.append(\"invalid email\")\n",
        "        \n",
        "        # Validate age\n",
        "        try:\n",
        "            age = int(row.get(\"age\", \"\"))\n",
        "            if age <= 0:\n",
        "                issues.append(\"age must be positive\")\n",
        "        except ValueError:\n",
        "            issues.append(\"age not a number\")\n",
        "        \n",
        "        if issues:\n",
        "            errors.append({\"line\": line_num, \"reasons\": issues, \"row\": row})\n",
        "        else:\n",
        "            valid.append(row)\n",
        "    \n",
        "    return {\"valid\": valid, \"errors\": errors}\n",
        "\n",
        "def merge_pages_solution(pages: List[Dict]) -> List[Dict]:\n",
        "    seen_ids = set()\n",
        "    result = []\n",
        "    \n",
        "    for page in pages:\n",
        "        for record in page.get(\"data\", []):\n",
        "            if record[\"id\"] not in seen_ids:\n",
        "                seen_ids.add(record[\"id\"])\n",
        "                result.append(record)\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Test solutions\n",
        "print(\"CSV Validation:\")\n",
        "csv_result = validate_csv_solution(SAMPLE_CSV)\n",
        "print(f\"  Valid: {len(csv_result['valid'])} rows\")\n",
        "print(f\"  Errors: {csv_result['errors']}\")\n",
        "\n",
        "print(\"\\nPage Merger:\")\n",
        "print(f\"  Result: {merge_pages_solution(PAGES)}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
