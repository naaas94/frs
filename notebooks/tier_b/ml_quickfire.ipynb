{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tier B: ML Quickfire Cards\n",
        "\n",
        "**Goal**: 60-second automatic answers for credibility questions.\n",
        "\n",
        "These are not \"gotcha\" questions - interviewers use them to verify you've done real ML work.  \n",
        "Memorize the ONE-LINER, understand the CONTEXT.\n",
        "\n",
        "---\n",
        "\n",
        "## Card 1: Leakage vs Overfitting\n",
        "\n",
        "### One-Liner\n",
        "> **Leakage** = using future/test info during training (data problem).  \n",
        "> **Overfitting** = model memorizes training data, fails on new data (model problem).\n",
        "\n",
        "### How to Detect\n",
        "| Issue | Detection |\n",
        "|-------|-----------|\n",
        "| Leakage | Suspiciously high validation score, drops in production |\n",
        "| Overfitting | Training acc >> validation acc (gap grows with complexity) |\n",
        "\n",
        "### Quick Fixes\n",
        "- **Leakage**: Audit feature pipeline, check temporal splits, remove target-derived features\n",
        "- **Overfitting**: Regularization, dropout, early stopping, more data, simpler model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Card 2: ROC-AUC vs PR-AUC\n",
        "\n",
        "### One-Liner\n",
        "> **ROC-AUC**: Use when classes are balanced, care about overall ranking.  \n",
        "> **PR-AUC**: Use when positive class is rare (imbalanced), care about precision at different recalls.\n",
        "\n",
        "### The Key Insight\n",
        "ROC-AUC can look great (0.95+) even when your model is useless on rare positives.  \n",
        "PR-AUC is harsh and honest about performance on the minority class.\n",
        "\n",
        "### Decision Rule\n",
        "```\n",
        "if positive_rate < 10%:\n",
        "    use PR-AUC\n",
        "else:\n",
        "    ROC-AUC is fine\n",
        "```\n",
        "\n",
        "### Visual Intuition\n",
        "- **ROC**: TPR vs FPR (can hide poor precision in imbalanced data)\n",
        "- **PR**: Precision vs Recall (directly shows what you care about)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Card 3: Calibration\n",
        "\n",
        "### One-Liner\n",
        "> **Calibration** = when model says \"70% probability\", it should be correct 70% of the time.\n",
        "\n",
        "### Why It Matters\n",
        "- Uncalibrated models give **rankings**, not real probabilities\n",
        "- Critical for: risk assessment, decision thresholds, combining predictions\n",
        "\n",
        "### How to Check\n",
        "- **Reliability diagram**: Plot predicted prob vs actual frequency in bins\n",
        "- Perfect calibration = diagonal line\n",
        "\n",
        "### How to Fix\n",
        "| Method | When to Use |\n",
        "|--------|-------------|\n",
        "| Platt Scaling | Binary, need sigmoid fit on validation set |\n",
        "| Isotonic Regression | More flexible, needs more data |\n",
        "| Temperature Scaling | Neural nets, single parameter |\n",
        "\n",
        "### Code Snippet\n",
        "```python\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "calibrated = CalibratedClassifierCV(model, method='isotonic', cv=5)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Card 4: Class Imbalance Levers\n",
        "\n",
        "### One-Liner\n",
        "> Multiple tools exist; choose based on problem constraints and where in the pipeline you can intervene.\n",
        "\n",
        "### The Toolkit (in order of preference)\n",
        "\n",
        "| Lever | How | When |\n",
        "|-------|-----|------|\n",
        "| **Threshold tuning** | Adjust decision threshold post-training | Always try first, no retraining |\n",
        "| **Class weights** | `class_weight='balanced'` | Built into most models, easy |\n",
        "| **Evaluation metrics** | PR-AUC, F1, balanced accuracy | Ensures you measure the right thing |\n",
        "| **Undersampling majority** | Random or Tomek links | Fast, loses information |\n",
        "| **Oversampling minority** | SMOTE, ADASYN | Can overfit, synthetic artifacts |\n",
        "| **Focal Loss** | Down-weight easy negatives | Deep learning, well-calibrated |\n",
        "| **Collect more data** | Active learning on minority | Best long-term, expensive |\n",
        "\n",
        "### Red Flags\n",
        "- Never SMOTE the test set\n",
        "- Threshold tuning beats resampling 80% of the time\n",
        "- Metrics matter more than tricks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Card 5: Slice-Based Error Analysis\n",
        "\n",
        "### One-Liner\n",
        "> Overall metrics hide where your model fails. Slice by meaningful subgroups to find systematic errors.\n",
        "\n",
        "### The Process\n",
        "1. **Define slices**: demographics, input features, data sources, edge cases\n",
        "2. **Compute metrics per slice**: precision, recall, error rate\n",
        "3. **Find underperformers**: slices where performance << overall\n",
        "4. **Diagnose root cause**: data quality? feature coverage? distribution shift?\n",
        "5. **Fix targeted**: more data for slice, specific features, or model ensemble\n",
        "\n",
        "### Common Slices\n",
        "- User segments (new vs returning, geo, device)\n",
        "- Input characteristics (length, language, category)\n",
        "- Temporal (weekday/weekend, hour, seasonality)\n",
        "- Label source (manual vs auto-labeled)\n",
        "\n",
        "### Tools\n",
        "- Pandas groupby + custom metrics\n",
        "- `slicefinder` library\n",
        "- ML monitoring platforms (Arize, Fiddler, WhyLabs)\n",
        "\n",
        "### Interview Answer Template\n",
        "> \"I slice the data by [meaningful dimension], compute [metric] per slice, identify underperforming segments, then investigate whether it's a data coverage or feature gap.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Quick Self-Test\n",
        "\n",
        "Can you answer each in under 60 seconds?\n",
        "\n",
        "1. \"Your model has 0.98 AUC but performs poorly in production. What's happening?\"\n",
        "2. \"When would you use PR-AUC over ROC-AUC?\"\n",
        "3. \"How do you know if your model's probabilities are calibrated?\"\n",
        "4. \"You have 1% positive rate. Walk me through your approach.\"\n",
        "5. \"How do you find where your model is failing?\"\n",
        "\n",
        "**If you hesitate on any, re-read that card.**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
