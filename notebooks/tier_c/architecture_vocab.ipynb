{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tier C: Architecture Vocabulary\n",
        "\n",
        "**Goal**: Recognition-level familiarity. You don't need to implement these from scratch - you need to discuss tradeoffs intelligently.\n",
        "\n",
        "These topics signal \"I've worked on production ML systems\" without requiring deep expertise.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "### What It Is\n",
        "LLM + external knowledge retrieval. Instead of fine-tuning, you retrieve relevant docs and stuff them into the prompt.\n",
        "\n",
        "### Architecture\n",
        "```\n",
        "Query → Embed → Vector Search → Top-K Docs → Prompt + Docs → LLM → Response\n",
        "```\n",
        "\n",
        "### Key Tradeoffs\n",
        "\n",
        "| Decision | Options | Tradeoff |\n",
        "|----------|---------|----------|\n",
        "| Chunk size | Small (256) vs Large (1024) | Precision vs Context |\n",
        "| Retriever | Dense (embeddings) vs Sparse (BM25) | Semantic vs Keyword match |\n",
        "| Top-K | Few (3) vs Many (10) | Focus vs Coverage |\n",
        "| Reranking | None vs Cross-encoder | Latency vs Accuracy |\n",
        "\n",
        "### Red Flags to Mention\n",
        "- \"Retrieval quality matters more than model size\"\n",
        "- \"Chunking strategy is often the biggest lever\"\n",
        "- \"Hybrid retrieval (dense + sparse) usually wins\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Agents (Tool Use / ReAct)\n",
        "\n",
        "### What It Is\n",
        "LLM that can take actions: call APIs, run code, query databases. Iterates: think → act → observe → repeat.\n",
        "\n",
        "### Core Pattern (ReAct)\n",
        "```\n",
        "Thought: I need to find the user's order status\n",
        "Action: call_api(\"orders\", user_id=123)\n",
        "Observation: {\"status\": \"shipped\", \"eta\": \"2024-01-15\"}\n",
        "Thought: I have the info, now I can respond\n",
        "Answer: Your order shipped and arrives Jan 15.\n",
        "```\n",
        "\n",
        "### Key Tradeoffs\n",
        "\n",
        "| Decision | Options | Tradeoff |\n",
        "|----------|---------|----------|\n",
        "| Tool selection | LLM chooses vs Router model | Flexibility vs Reliability |\n",
        "| Max iterations | Few (3) vs Many (10) | Cost/latency vs Completeness |\n",
        "| Error handling | Retry vs Fallback vs Human | Autonomy vs Safety |\n",
        "| Memory | None vs Conversation vs Long-term | Simplicity vs Continuity |\n",
        "\n",
        "### Red Flags to Mention\n",
        "- \"Agents are expensive (many LLM calls) - use sparingly\"\n",
        "- \"Tool descriptions are critical - garbage in, garbage out\"\n",
        "- \"Always have a max-iteration cap and cost budget\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Observability (ML Systems)\n",
        "\n",
        "### What It Is\n",
        "Monitoring + debugging for ML in production. Three pillars: metrics, logs, traces.\n",
        "\n",
        "### What to Track\n",
        "\n",
        "| Layer | Metrics |\n",
        "|-------|---------|\n",
        "| **Infra** | Latency p50/p99, throughput, error rate, GPU util |\n",
        "| **Model** | Prediction distribution, confidence scores, feature drift |\n",
        "| **Business** | Conversion, user actions, downstream impact |\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "- **Data Drift**: Input distribution changed from training\n",
        "- **Concept Drift**: Relationship between inputs and outputs changed\n",
        "- **Shadow Mode**: Run new model in parallel, compare without serving\n",
        "- **A/B Testing**: Split traffic, measure business metrics\n",
        "\n",
        "### Tools to Name-Drop\n",
        "- Metrics: Prometheus, Datadog, CloudWatch\n",
        "- ML-specific: Arize, Fiddler, WhyLabs, MLflow\n",
        "- Tracing: Jaeger, Honeycomb (for LLM chains)\n",
        "\n",
        "### Red Flags to Mention\n",
        "- \"Accuracy in prod != accuracy in training\"\n",
        "- \"Monitor input distributions, not just model outputs\"\n",
        "- \"Alert on distribution shift before accuracy drops\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Deployment Patterns\n",
        "\n",
        "### Serving Modes\n",
        "\n",
        "| Mode | When | Example |\n",
        "|------|------|---------|\n",
        "| **Batch** | Precompute, latency tolerant | Recommendations computed nightly |\n",
        "| **Online** | Real-time, per-request | Fraud detection at checkout |\n",
        "| **Streaming** | Continuous, event-driven | Real-time personalization |\n",
        "\n",
        "### Optimization Techniques\n",
        "\n",
        "| Technique | What It Does | Tradeoff |\n",
        "|-----------|--------------|----------|\n",
        "| **Batching** | Group requests, amortize overhead | Latency vs Throughput |\n",
        "| **Caching** | Store repeated predictions | Memory vs Compute |\n",
        "| **Quantization** | Reduce precision (FP32→INT8) | Size/Speed vs Accuracy |\n",
        "| **Distillation** | Train small model on large model outputs | Complexity vs Performance |\n",
        "| **Model sharding** | Split model across GPUs | Enables large models |\n",
        "\n",
        "### A/B Testing Essentials\n",
        "- Traffic splitting (usually 5-10% to new model)\n",
        "- Statistical significance (not just \"looks better\")\n",
        "- Guardrail metrics (things that must not get worse)\n",
        "- Rollback capability\n",
        "\n",
        "### Red Flags to Mention\n",
        "- \"Start with caching - it's often 10x improvement for free\"\n",
        "- \"Batch size tuning is underrated for throughput\"\n",
        "- \"Always have a kill switch for new models\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Quick Recognition Test\n",
        "\n",
        "If the interviewer mentions these, can you respond with 1-2 intelligent sentences?\n",
        "\n",
        "1. \"How would you add RAG to this system?\"\n",
        "2. \"Have you worked with LLM agents?\"\n",
        "3. \"How do you monitor ML models in production?\"\n",
        "4. \"What's your deployment strategy for a new model?\"\n",
        "\n",
        "**Goal**: Sound like you've done this before, not like you're reciting a textbook.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
